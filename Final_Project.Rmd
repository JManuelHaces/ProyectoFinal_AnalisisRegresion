---
title: "Final Project"
output: html_notebook
---

```{r, message=FALSE, warning=FALSE}
# Libraries
library(renv)
library(RODBC)
library(boot)
library(lares)
library(psych)
library(lme4)
library(olsrr)
library(lmtest)
library(ggpubr)
library(ggplot2)
library(survival)
library(ggfortify)
library(survminer)
library(tidyverse)
library(ggfortify)
library(DescTools)
library(ggfortify)
library(ggcorrplot)
library(tidymodels)
library(patchwork)
```

# Phase 1 - (40%)

-   The company want to create a model that predicts the probability of a New Client to repurchase in the Month 5, based on their transactional behavior during the first month (considering the first month as M0).

```{r}
# Loading dataset with the first month activity for every client
data1 <- readxl::read_excel("./Data/clients_first_month_VF.xlsx", sheet = "db")

# Changing the variables types
data1$id <- as.numeric(data1$id)
data1$age_gen <- as.factor(data1$age_gen)
data1$date_order <- as.Date(data1$date_order)
data1$MXN <- as.numeric(data1$MXN)
data1$products <- as.numeric(data1$products)

head(data1)
```

```{r}
# Loading dataset with the first month activity for every client
data2 <- readxl::read_excel("./Data/clients_18months_VF.xlsx", sheet = "db")

# Changing the variables types
data2$id <- as.numeric(data2$id)
data2$month <- as.factor(data2$month)
data2$MXN <- as.numeric(data2$MXN)
  
head(data2)
```

## 1.1) Aditional variables to calculate:

-   Recency: Days since last order considerig the end of the period as 31/03/2021.
-   Frequency: Number of orders in the first month.
-   Order Size: AVG MXN of the orders in their first month
-   Total MXM: Total MXN spend in the first month.

```{r, message=FALSE}
# Making the variables for the first dataset
data1 <- data1 %>% 
  group_by(id, market, age_gen) %>% 
  summarise(max_month = max(date_order),
            recency = as.numeric(difftime(as.Date("2021-03-31"), max_month, units = "days")),
            frequency = as.numeric(n()),
            order_size = as.numeric(mean(MXN)),
            total_MXN = as.numeric(sum(MXN))
            )

data1 <- data1 %>% select(-max_month)
data1$market <- as.factor(data1$market)
summary(data1)
```

```{r}
# Making the objective variable
data1_5 <- data2 %>% 
  filter(month == "M5") %>% 
  mutate(status = ifelse (MXN > 0, 1, 0))

# Selecting the columns that we need
data1_5 <- data1_5 %>% select(id, status)
data1_5$status <- as.factor(data1_5$status)
data1_5
```

```{r}
# Merging both dataset
data1 <- merge(data1, data1_5, by="id")
data1
```

## 1.2) Develop a data exploration.

### 1.2.1) Categorical variables

```{r}
categoric_df <- data1 %>% select(market,age_gen)
summary(categoric_df)
```

-   We have only two categorical variables without null values and with a low granularity.

```{r}
initial_props <- data1 %>% count(status) %>% mutate(Balance= n/sum(n)) %>% filter(status==1)
initial_props
```

The percentage of customers who repurchased in the entire data set itÂ´s around 40.71%, now we are going to compare this proportion to all the categories of our categorical variables.

```{r}
market_props <- data1 %>% group_by(market) %>%  count(status) %>% mutate(proportion = n/sum(n)) %>% filter(status==1)

ggplot(market_props,aes(x=market, y=proportion)) + geom_point(size = 3.7, colour= c("blue", "red")) + geom_hline(yintercept=0.4079265, linetype="dashed", color = "red", linetype = 'Proportion of all data') + 
                                          labs( title = 'Percentage of clients who repurchased',
                                          x = 'Categories of the "Market" variable',
                                          y = "%") 
```

The red dashed line is the proportion of the entire data set and as we can see the English category of the market variable is pretty close to it and doesn't seem to be a very good predictor, on the other hand the Spanish may be more useful in order to predict customers who are not going to repurchase due to it has a lower proportion on people who repurchased comparing to the entire data set.

```{r}
gen_props <- data1 %>% group_by(age_gen) %>%  count(status) %>% mutate(proportion = n/sum(n)) %>% filter(status==1)

ggplot(gen_props,aes(x=age_gen, y=proportion)) + 
  geom_point(size = 3.7, colour= c("blue", "red", 'Brown','Orange')) + 
  geom_hline(yintercept=0.4079265, linetype="dashed", color = "red", linetype = 'Proportion of all data') + 
  labs( title = 'Percentage of clients who repurchased',
        x = 'Categories of the "Age Generation" variable',
        y = "%") 
```

Analyzing the second categorical variable and based on the previous visualization there is a good chance that "Boomers" and "Generation Z" end up being good predictors because of it's difference on the proportion of customers who repurchased compared to the other categories and the general proportion.

### 1.2.2) Numerical variables

```{r}
numeric_df <- data1 %>% select(-c(market,age_gen,status))
summary(numeric_df)

```

Recency:

-   The 50% of the clients had 4 days since last order considerig the end of the period as 31/03/2021
-   The maximum value for this variable is 30, from this moment we may think that customers with values like this one could have less probabilities to repurchase

```{r}
# Layout to split the screen
layout(mat = matrix(c(1,2),2,1, byrow=TRUE),  height = c(1,8))
 
# Draw the boxplot and the histogram 
par(mar=c(0, 3.1, 1.1, 2.1))
boxplot(numeric_df$recency, horizontal=TRUE , ylim=c(0,30), xaxt="n" , col=rgb(0.8,0.8,0,0.5) , frame=F)
par(mar=c(4, 3.1, 1.1, 2.1))
hist(numeric_df$recency , breaks=40 , col=rgb(0.2,0.8,0.5,0.5) , border=F , main="" , xlab="Days since last order", xlim=c(0,30))
```

Frecuency:

-   75% of the customers had 3 or less number of orders in the first month.

```{r}
# Layout to split the screen
layout(mat = matrix(c(1,2),2,1, byrow=TRUE),  height = c(1,8))
 
# Draw the boxplot and the histogram 
par(mar=c(0, 3.1, 1.1, 2.1))
boxplot(numeric_df$frequency, horizontal=TRUE , ylim=c(0,30), xaxt="n" , col=rgb(0.8,0.8,0,0.5) , frame=F)
par(mar=c(4, 3.1, 1.1, 2.1))
hist(numeric_df$frequency , breaks=40 , col=rgb(0.2,0.8,0.5,0.5) , border=F , main="" , xlab="Number of orders in the first month.", xlim=c(0,120))
```

Order Size:

-   75% of the customers had an average of \$175.59 or less in their orders in the first month.
-   As we can see The mean is greater more than 50% than the median, and we have a maximum value of \$1012, that's probabliy because we could have outliers in that variable

```{r}
# Layout to split the screen
layout(mat = matrix(c(1,2),2,1, byrow=TRUE),  height = c(1,8))
 
# Draw the boxplot and the histogram 
par(mar=c(0, 3.1, 1.1, 2.1))
boxplot(numeric_df$order_size, horizontal=TRUE , ylim=c(0,1012), xaxt="n" , col=rgb(0.8,0.8,0,0.5) , frame=F)
par(mar=c(4, 3.1, 1.1, 2.1))
hist(numeric_df$order_size , breaks=40 , col=rgb(0.2,0.8,0.5,0.5) , border=F , main="" , xlab="Avg of money by orders in the first month", xlim=c(0,1012))
```

Total MXN:

-   75% of the customers spend \$422.6 or less in their first month.
-   The mean is greater more than 70% than the median, and we have a maximum value of 4047, we may have outliers in this variable too.

```{r}
# Layout to split the screen
layout(mat = matrix(c(1,2),2,1, byrow=TRUE),  height = c(1,8))
 
# Draw the boxplot and the histogram 
par(mar=c(0, 3.1, 1.1, 2.1))
boxplot(numeric_df$total_MXN, horizontal=TRUE , ylim=c(0,4047), xaxt="n" , col=rgb(0.8,0.8,0,0.5) , frame=F)
par(mar=c(4, 3.1, 1.1, 2.1))
hist(numeric_df$total_MXN , breaks=40 , col=rgb(0.2,0.8,0.5,0.5) , border=F , main="" , xlab="Total money spent in the first month", xlim=c(0,4047))
```

The most correlated variables are order_size + total_MXN which makes sense as the order size increase, the cost of it increases too. We are not going to remove any of those highly correlated variables as we only have a few variables to work with and they are not correlated enough in order to think that they are going to bring us problems in our model.

Top correlations

```{r}
corr_cross(select(data1,recency,frequency,order_size,total_MXN,status), rm.na = T, max_pvalue = 0.05, top = 15, grid = T)
```

From this correlation analysis the numeric variable most probable to have influence in our model is recency.

## 1.3) Develop a logistic regression in order to solve the problem. (The split of train and test is up to you)

```{r}
# Response variable balance
data1 %>% count(status) %>% mutate(Balance= n/sum(n))
```

```{r}
# Train and test
set.seed(10)

split_inicial<- initial_split(
                data = data1 %>% select(-(id)),
                prop = 0.8,
                strata = status
                )

train <- training(split_inicial)
test <- testing(split_inicial)

# Training the model
logistic <- glm(status ~ ., data=train, family="binomial")
summary(logistic)
```

-   Interpret the coefficients of your model.

    -   *marketSpanish* -> If the client is from the Spanish Market, the probability of repurchase decrease by 28% - This variable is *statistically significant*

    -   *age_genGeneration X* -> If the client is generation X, the probability of repurchase increases by 9% - However it is *not statistically significant*

    -   *age_genGeneration Z* -> If the client is generation Z, the probability of repurchase increases by 12% - However it is *not statistically significant*

    -   *age_genMillennials* -> If the client is generation Millenials, the probability of repurchase increases by 7% - However it is *not statistically significant*

    -   *recency* -> For every day that the last purchase of the client is from today, the chances of repurchase decreses by -0.5267 times. - This variable is *statistically significant*

    -   *frequency* -> For every order the client made in the first month, the probability of repurchase decreases by -0.01771 times. - However it is *not statistically significant*

    -   *order_size* -> For every 'peso' that the mean of purchases in the first month increase, the chances of repurchase decreases by -0.00051 times. - However it is *not statistically significant*

    -   *total_MXN* -> For each peso that the customer's total purchase increases in the first month, the chances of repurchase increases by 0.00097 times. - This variable is *statistically significant*



-   Review the performance of the model on the test data (Accuracy, Sensitivity, Specificity, Confusion Matrix)

```{r}
test_results <- test
test_results$prob_model <- logistic%>% predict(test, type = "response")
    test_results$class_model <- as.factor(ifelse(test_results$prob_model >= 0.5, 1, 0))
# Accuracy
acc_model <-  accuracy(data = test_results,
                       truth    = status,
                       estimate = class_model)
acc_model <-acc_model$.estimate
print(paste0('Accuracy: ', acc_model))

# Confusion Matrix
cm_model <- test_results %>% 
  conf_mat(
    truth=status,
    estimate=class_model
    )

# Sensitivity
sen_model <- cm_model$table[4]/(cm_model$table[4]+cm_model$table[3])
print(paste0('Sensitivity: ', sen_model))

# Specificity
spe_model <- cm_model$table[1]/(cm_model$table[1]+cm_model$table[2])
print(paste0('Specificity: ', spe_model))

print('Confusion Matrix:')
cm_model
```

    -   Consider as threshold 40%, 50% and 60%, present the performance for all the thresholds and select the best one (Justify your decision)

```{r}
# 40% threshold
test_results_40 <- test
test_results_40$prob_model <- logistic%>% predict(test, type = "response")
test_results_40$class_model <- as.factor(ifelse(test_results_40$prob_model >= 0.4, 1, 0))

# 50% threshold
test_results_50 <- test
test_results_50$prob_model <- logistic%>% predict(test, type = "response")
test_results_50$class_model <- as.factor(ifelse(test_results_50$prob_model >= 0.5, 1, 0))

# 60% threshold
test_results_60 <- test
test_results_60$prob_model <- logistic%>% predict(test, type = "response")
test_results_60$class_model <- as.factor(ifelse(test_results_60$prob_model >= 0.6, 1, 0))
```

Results:

```{r}
print("Treshold 40:")
# Confusion Matrix
cm_model_40 <- test_results_40 %>% conf_mat(
  truth=status,
  estimate=class_model)

# Accuracy
acc_model_40 <-  accuracy(data = test_results_40, truth = status, 
                       estimate = class_model)$.estimate

print(paste0('Accuracy: ', acc_model_40))

# Sensitivity
sen_model_40 <- cm_model_40$table[4]/(cm_model_40$table[4]+cm_model_40$table[3])
print(paste0('Sensitivity: ', sen_model_40))

# Specificity
spe_model_40 <- cm_model_40$table[1]/(cm_model_40$table[1]+cm_model_40$table[2])
print(paste0('Specificity: ', spe_model_40))
print("-----------------------------------------------------------------------")


print("Treshold 50:")
# Confusion Matrix
cm_model_50 <- test_results_50 %>% conf_mat(
  truth=status,
  estimate=class_model)

# Accuracy
acc_model_50 <-  accuracy(data = test_results_50, truth = status, 
                       estimate = class_model)$.estimate

print(paste0('Accuracy: ', acc_model_50))

# Sensitivity
sen_model_50 <- cm_model_50$table[4]/(cm_model_50$table[4]+cm_model_50$table[3])
print(paste0('Sensitivity: ', sen_model_50))

# Specificity
spe_model_50 <- cm_model_50$table[1]/(cm_model_50$table[1]+cm_model_50$table[2])
print(paste0('Specificity: ', spe_model_50))

print("-----------------------------------------------------------------------")

print("Treshold 60:")
# Confusion Matrix
cm_model_60 <- test_results_60 %>% conf_mat(
  truth=status,
  estimate=class_model)

# Accuracy
acc_model_60 <-  accuracy(data = test_results_60, truth = status, 
                       estimate = class_model)$.estimate

print(paste0('Accuracy: ', acc_model_60))

# Sensitivity
sen_model_60 <- cm_model_60$table[4]/(cm_model_60$table[4]+cm_model_60$table[3])
print(paste0('Sensitivity: ', sen_model_60))

# Specificity
spe_model_60 <- cm_model_60$table[1]/(cm_model_60$table[1]+cm_model_60$table[2])
print(paste0('Specificity: ', spe_model_60))
```
Conclusion:
- The treshold that we chose is '50', we chose this one because it has the best 'Accuracy' and it has a better balanced sensitivity and specificity.


-   Once that the threshold is selected, calculate the probability of repurchase for all the dataset (train and test) and based on the probability classify the new clients:

    -   Above or equal to the threshold selected as "High Probability"
    -   Less than 15% as "Low Probability"
    -   The rest as "Medium Probability"
```{r}
# Selected Treshold
treshold_selec <- 0.5

# Removing the response
data_label <- data1
data_label <- data_label %>% select(-(status))

# Puting the probability into the dataset
data_label$Prob_Predicted <- logistic %>% predict(data_label, type = "response")

# Making a Label for each case
data_label$Prob_Predicted <- case_when(
    data_label$Prob_Predicted >= treshold_selec ~ "High Probability",
    data_label$Prob_Predicted <= 0.15 ~ "Low Probability",
    TRUE ~ "Medium Probability"
)

data_label
```


# Phase 2 - (40%)

-   Generate a survival analysis for the new clients.

```{r}
data_2 <- readxl::read_excel("./Data/clients_18months_VF.xlsx")
data_2$month <- as.factor(data_2$month)

# Create numeric month variable
data_2$month_num <- sub("M", "", data_2$month)
data_2$month_num <- as.numeric(data_2$month_num)
head(data_2)
```

```{r}
summary(data_2)
```

-   Consider as "dead" those who do not make a purchase in the month 18.

```{r}
data_fin <- data_2 %>% filter(MXN>0) %>% group_by(id)  %>%  
  summarise(time_surv = max(month_num)) %>% 
  mutate(status =case_when(
         time_surv == 17 ~ 0,
         TRUE ~ 1))

data_fin
```

-   Use the variables, age generation, market, probability label and recency and interpret the coeficients.

```{r}
# First, we have to merge the dataset with the probability labels with this dataset.
data_fin <- merge(data_label, data_fin)

# Now we apply a filter to keep the mention variables.
data_fin <- data_fin %>% select(c(id, age_gen, market, Prob_Predicted, recency, time_surv,status))
data_fin
```

```{r}
data_fin$Prob_Predicted <- as.factor(data_fin$Prob_Predicted)
summary(data_fin)
```
### Model
```{r}
model <- coxph(Surv(time_surv, status) ~ age_gen + market + Prob_Predicted + recency, data = data_fin)
summary(model)
```
The 2 variables that we can interpretate are:

Prob_PredictedMedium Probability -> The probability of not surviving increases 1.3236 times if you belong to the medium probability prediction label.

recency -> The bigger it is the recency the probability of not surviving increases 1.0272 times 


## Generate the survival curves based on the following.

### Graph 1 - Based on probability label

```{r}
ggsurvplot(survfit(Surv(time_surv, status) ~ Prob_Predicted , data = data_fin),data=data_fin)
```
The prediction performs well, low probability label is the one that survives the less, followed by medium label. The high probability label is the one that survives the most.


### Graph 2 - Based on market

```{r}
ggsurvplot(survfit(Surv(time_surv, status) ~ market , data = data_fin),data=data_fin)
```

It doesn't seem to be a difference between both markets.

These graphs show that the company should campaign with people who have the label of low and medium probability.